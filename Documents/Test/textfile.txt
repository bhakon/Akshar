ABSTRACT :-
As the massive amounts of both structured and unstructured data produced daily continues to grow , there are lot of actions that we do on data takes much time and consume more human resources.  These tasks can also be performed by an trained program that can do the same work with in less time.  However, as artificial intelligence technologies allow computers to track, extract, and analyse the vast array of information.  One of these technologies makes use of a set of algorithms known as Natural Language Processing and deep learning, which allows computers to analyse the content of natural human language and derive new information from it. 

This application is an one stop solution for most of the text related tasks that we encounter daily on our large corpus of data which include large articles , text documents etc.  It is a web application that manages user given data and aids users to grasp much information and get valuable insights of their data.  The application provides textual summary of his input in both extractive and abstractive ways.  This summaries can also preserve sentiments and semantics of the context which can be further utilized for product reviews , users opinion thus one can generate reliable ratings of the products. Also, the application generates visualizations of your textual data ( through word cloud representations , statistics of different kinds of words ) , keywords extraction etc.  for gaining better knowledge.  Automatic Text Summarization will present one or more text documents while maintaining the main information content using an automatic machine with no more than half the original text or less than the original text.  The extractive approach is still in demand in the past three years because the extractive is easier than abstractive.  Users can also query their own text for answers or just give an question to the application which can try to fetch relevant answers.  



Motivation Of The Project :-

The idea of the project came from the hectic work of understanding large text corpus in least possible time with out having much efforts for trivial tasks and getting as much information as possible.  Through this platform we want to give users the essence of their inputs to make their work easier. 

The application generates summaries of the text which is a subtext that does not exceed more than half of the length of the input text often less than half of it. It captures the main idea of the text and removes redundancies if any.  It helps readers quickly identify information of their interest. There are many potential applications for this some of them could be :

 Abstracts ( of documents )
 Head lines ( for news articles)
 Table of contents ( of a large document)
 Outlines ( notes for students )
 Minutes ( of a meeting )
 Previews ( of movies )

And so on …

We even want to give the text a visual feeling for better interactivity with the text to the user. we want to automate many things that we do on text daily.  We want to leverage the power of natural language processing and deep learning and make lives easier.     

 Problem Definition :-
This project is based on shrinking the text to the extent that is useful for the user.  The aim is to generate summaries by keeping sentence relativity , meaning and mainly important and required information form the original text , also to request answers to the questions generated by users , to make some pictures out of text that will help more for us to get into it.  


 Objectives  :-
Through this project we want to achieve the following objectives :

 The model must summarize the given corpus of text ( of any length). 
 Able to summarize the review / comments by preserving the sentiments. 
 Help visualizing the text for the user. 
 Keywords extraction.  
 Give short information upon user’s search query. 
 In document Query resolution. 
 

Goals :-
 We want to achieve human level performance for the stated objectives. 
 Better user experience with the platform. 
 Reduce human efforts for trivial tasks. 
 Ease of use.  
 Easy extraction of information.  

Automatic text Summarization (ATS) is nowadays a popular research area among researchers.  Automatic text Summarization is the approach of generating the subset of the main text.  This subset of the main text represents the complete text and the main idea of the text.  Automatic Text Summarization is also known as Test Summarization.  ATS is the important field of Natural Language Processing (NLP) and Data Mining (DM).  This includes the abstractive and extractive summaries of the text.  
With the advancement of technology, the internet is accessible through various devices like smart phones, smart watches and within the reach of common people.  That leads to the accessibility of lot of information through world wide web (WWW).  More information on the internet sometimes it becomes so difficult to select only required information from large texts.  Due to the information, manual summarization of information is very challenging and also time-consuming task.  Thus, we need an automatic text summarization system. Summary is a text that is produced from one or more texts, that conveys the important information text(s), and that is no longer than half of the original text(s) and usually significantly less than that.  Summaries make the task of understanding the meaning of text easier.  Text summarization helps user to manage vast amount of information by condensing document and include more relevant facts into them.  
The basic concept of the Automatic text summarization process based on literature review can be divided into 3 types of text summarization, namely numbers of documents, namely single document and multi document, techniques, namely extractive and abstractive, classification based, namely supervised and unsupervised.  


Single Document
A single document summary system will produce a summary based on one document source.  A single document can consist of several sub documents with several paragraphs.  The content described in each sub-document emphasizes all the different aspects around the same topic. 
Extractive Summarization 
Extractive summarization is extraction based summarization whose summary consists entirely of extracted content.  Initially the research centered on techniques for managing documents with several approaches such as based on sentence position or word frequency in text.  The experiment was then carried out using the Information Extraction (IE) Extraction technique for automatic summarizing on the grounds of increased accuracy and more specific results.  
 Abstractive Summarization
Abstractive summarization is a summarizing system by producing new phrases or using words that are not in the original text.  For perfect abstractive summaries, the model must really understand the document and then try to express that understanding briefly using new words and phrases or arrange them in different forms.  The extract field is more well-researched, in contrast to abstracts which have more challenging problems and require extensive natural language processing. 
 Keywords Extraction 
Keyword extraction (also known as keyword detection or keyword analysis) is a text analysis technique that automatically extracts the most used and most important words and expressions from a text.  It helps summarize the content of texts and recognize the main topics discussed. 
Text analysis uses machine learning artificial intelligence (AI) with natural language processing (NLP) to break down human language so that it can be understood and analyzed by machines.  Keyword analysis can find keywords from all manner of text: regular documents and business reports, social media comments, online forums and reviews, news reports, and more. 
To analyze thousands of online reviews about your product.  Keyword extraction helps you sift through the whole set of data and obtain the words that best describe each review in just seconds.  That way, you can easily and automatically see what your customers are mentioning most often, saving your teams hours upon hours of manual processing. 

Word cloud representations
Word Clouds (also known as wordle, word collage or tag cloud) are visual representations of words that give greater prominence to words that appear more frequently.  For Mentimeter Word Clouds, the words that are added most frequently by audience members using their smartphones.  This type of visualization can help presenters to quickly collect data from their audience, highlight the most common answers and present the data in a way that everyone can understand. 
Choosing the word can be based on 
 Frequency 
 Significance 
 Categorization
 Sentence Scoring based on Word Frequency
The first approach will explore the simplest of the three.  Here we assign weights to each word based on the frequency of the word in the passage.  For example, if “Soccer” occurs 4 times within the passage, it will have a weight of 4. 
Using the weights assigned to each word, we will create a score for each sentence.  In the end, we will be taking the score of the top `N` sentences for the summary.  As you’d imagine, just by leveraging the raw score of each sentence, the length of certain sentences will skew the results.  This is why we will normalize the scores by dividing by the length of each sentence. 
Now, to create the summary, we will take any sentence that has a score that exceeds a threshold.  In this case, the threshold will be the average score of all of the sentences. 

 Text Rank approach
This is essentially a derivative of the famous PageRank created by the Google Cofounders.  In PageRank, they generated a matrix that calculates the probability that a user will move from one page to another.  In the case of TextRank, we generate a cosine similarity matrix where we have the similarity of each sentence to each other. 
A graph is then generated from this cosine similarity matrix.  We will then apply the PageRank ranking algorithm to the graph to calculate scores for each sentence. 
 Proposed Methods 
 Extractive text Summarization

Extractive text summarization done by picking up the most important sentences from the original text in the way that forms the final summary.  Extractive techniques generally generate summaries through 3 phases or it essentially based on them.  
These phases are preprocessing step, processing step and generation 
step:
 1) Preprocessing step: the representation space dimensionality of the original text is reduced to involve a new structure representation. 
It usually includes:
a. Stop-word elimination: Common words without semantics that do not collect information relevant to the task (for example, "the", "a", "an", "in") are eliminated. 
2)Processing step: It uses an algorithm with the help of features generated in the preprocessing step to convert the text structure to the summary structure.  In which, the sentences are scored. 
3)Generation step: sentences are ranked.  Then, it pick up the most important sentences from the ranked structure to generate the final required summary. 
The last two stages - processing and generation steps - can be also described approximately as three main components: sentence scoring, selection and paraphrasing (reformulation).  At sentence scoring, for each sentence a score is assigned which points to its significance.  After that, the most important sentences is extracted. .  At sentence selection, the summarization system has to specify the best collection of significant sentences that form the final summary with taking into consideration the most prominent factors: redundancy and cohesion.  The traditional method for sentence selection is to pick up the top ranked sentences directly but, the redundancy elimination is the key issue. 
Text Pre-processing

Text pre-processing can be divided into two broad categories — noise removal & normalization.  Data components that are redundant to the core text analytics can be considered as noise. 


Handling multiple occurrences / representations of the same word is called normalization.  There are two types of normalization — stemming and lemmatization.  Let us consider an example of various versions of the word , learned, learning, learner.  Normalisation will convert all these words to a single normalised version  “learn”. 
Stemming normalizes text by removing suffixes. 
Lemmatisation is a more advanced technique which works based on the root of the word. 

Removing stopwords:
Stop words include the large number of prepositions, pronouns, conjunctions etc in sentences.  These words need to be removed before we analyse the text, so that the frequently used words are mainly the words relevant to the context and not common words used in the text. 

Text preparation
Text in the corpus needs to be converted to a format that can be interpreted by the machine learning algorithms.  There are 2 parts of this conversion — Tokenisation and Vectorisation. 

Tokenisation is the process of converting the continuous text into a list of words.  The list of words is then converted to a matrix of integers by the process of vectorisation.  Vectorisation is also called feature extraction. 
For text preparation we use the bag of words model which ignores the sequence of the words and only considers word frequencies. 

As the first step of conversion, we will use the CountVectoriser to tokenise the text and build a vocabulary of known words.  We first create a variable “cv” of the CountVectoriser class, and then evoke the fit_transform function to learn and build the vocabulary. 

The next step of refining the word counts is using the TF-IDF vectoriser.  The deficiency of a mere word count obtained from the countVectoriser is that, large counts of certain common words may dilute the impact of more context specific words in the corpus.  This is overcome by the TF-IDF vectoriser which penalizes words that appear several times across the document.  TF-IDF are word frequency scores that highlight words that are more important to the context rather than those that appear frequently across documents. 
TF-IDF consists of 2 components:
TF — term frequency
IDF — Inverse document frequency

Based on the TF-IDF scores, we can extract the words with the highest scores to get the keywords for a document. 

Query resolution 
When ever user enters any query , with the help of python library that helps in accessing the links from the internet (these are the links that you get when you search the same query on google search engine. ) up on users request multiple webpages are scraped and text is collected form those websites.  Using our extractive or abstractive approaches we can get an answer for the users query. 

 Project methodology
A life cycle model represents all the methods required to make a software product transit through its life cycle stages.  It also captures the structure in which these methods are to be undertaken. In other words, a life cycle model maps the various activities performed on a software product from its inception to retirement.  Different life cycle models may plan the necessary development activities to phases in different ways.  Thus, no element which life cycle model is followed, the essential activities are contained in all life cycle models though the action may be carried out in distinct orders in different life cycle models.  During any life cycle stage, more than one activity may also be carried out. 
A software life cycle model describes entry and exit criteria for each phase.  A phase can begin only if its stage-entry criteria have been fulfilled.  So without a software life cycle model, the entry and exit criteria for a stage cannot be recognized.  Without software life cycle models, it becomes tough for software project managers to monitor the progress of the project. 

There are many types of SDLC models available , but we used the iterative model for our project. 
 Iterative Model
This model leads the software development process in iterations.  It projects the process of development in cyclic manner repeating every step after every cycle of SDLC process
The software is first developed on very small scale and all the steps are followed which are taken into consideration.  Then, on every next iteration, more features and modules are designed, coded, tested and added to the software.  Every cycle produces a software, which is complete in itself and has more features and capabilities than that of the previous one. 
After each iteration, we can do work on risk management and prepare for the next iteration.  Because a cycle includes small portion of whole software process, it is easier to manage the development process but it consumes more resources. 
Why we used the Iterative Model?
 Requirements are defined clearly and easy to understand. 
 The software application is large. 
 There are chances of requirement of changes in future. 
 Advantages of Iterative Model:
 Testing and debugging during smaller iteration is easy. 
 A Parallel development can plan. 
 It is easily acceptable to ever-changing needs of the project. 
 Risks are identified and resolved during iteration. 
 Limited time spent on documentation and extra time on designing. 




 Requirement Engineering
Requirements engineering (RE) refers to the process of defining, documenting, and maintaining requirements in the engineering design process.  Requirement engineering provides the appropriate mechanism to understand what the customer desires, analyzing the need, and assessing feasibility, negotiating a reasonable solution, specifying the solution clearly, validating the specifications and managing the requirements as they are transformed into a working system.  Thus, requirement engineering is the disciplined application of proven principles, methods, tools, and notation to describe a proposed system's intended behavior and its associated constraints. 

 Feasibility Study:
The objective behind the feasibility study is to create the reasons for developing the software that is acceptable to users, flexible to change and conformable to established standards. 
Technologically feasible: The language used is python and python idle is used as the platform which is very much popular and usable in most of the devices.  The idea is practically possible but uniquely presented through this project. 

Economically feasible: The project is totally a software project and needs only a platform for presentation.  There is no hardware involved in making the project as part of the project.  The project can run on a normal computer. 

Legally feasible: The data used for visualization will be taken with the permission of the candidates and doesn’t harm any organization.  The data used for summarization and visualization will be used for study and only project purpose. 

 Requirement Elicitation and Analysis
This is also known as the gathering of requirements.  
Analysis of requirements starts with requirement elicitation.  The requirements are analyzed to identify inconsistencies, defects, omission, etc.  We describe requirements in terms of relationships and also resolve conflicts if any. 
Requirements are:

 The model must summarize the given corpus of text ( of any length). 
 Able to summarize the review / comments by preserving the sentiments. 
 Help visualizing the text for the user. 
 Keywords extraction.  
 Give short information upon user’s search query. 


 System Analysis
At this step the developers decide a road map of their plan and try to bring up the best software model suitable for the project.  System analysis includes Understanding of software product limitations, learning system related problems or changes to be done in existing systems beforehand, identifying and addressing the impact of project on organization and personnel etc.  The project team analyzes the scope of the project and plans the schedule and resources accordingly. 

 Functional requirements
Outputs from computer systems are required primarily to communicate the results of processing to users.  They are also used to provide a permanent copy of the results for later consultation. These are the requirements that the end user specifically demands as basic facilities that the system should offer.  All these functionalities need to be necessarily incorporated into the system as a part of the contract.  These are represented or stated in the form of input to be given to the system, the operation performed and the output expected.  They are basically the requirements stated by the user which one can see directly in the final product, unlike the non-functional requirements.  
They are :-
 The application must accept most types of text related sources I. e pdf , word document , able to insert text etc. 
 It must able to scrape data neatly for the specified web links. 
 Able to give summaries extracted from text.  
 Able to write summaries in its own way also. 
 Must generate word clouds and histograms for the given text depending up on the specified requirements. 
 Extract keywords and phrases in the given text. 
 Highlight the extracted keywords and phrases. 
 Answer the queries asked by the user form his source . 
 If user did not give any source and still want to query , application must scrape and show the results from the internet. 
 Non - Functional requirements
 These are basically the quality constraints that the system must satisfy according to the project contract.  The priority or extent to which these factors are implemented varies from one project to other.  They are also called non-behavioral requirements. 
A non-functional requirement is essential to ensure the usability and effectiveness of the entire software system.  Failing to meet non-functional requirements can result in systems that fail to satisfy user needs. 

 Performance – for example Response Time, Throughput, Utilization
 Scalability
 Capacity
 Availability
 Reliability
 Recoverability
 Maintainability
 Serviceability
 Security
 Data Integrity
 Usability
 Interoperability



 HARDWARE REQUIREMENTS

The hardware requirement specifies each interface of the software elements and the hardware elements of the system.  These hardware requirements include configuration characteristics. 

System						:				 Any processor after Pentium
Hard Disk					:				 40GB. 
Monitor					:				14’ColourMonitor. 
Mouse						:				Optical Mouse. 
Ram						:				512 Mb. 

 SOFTWARE REQUIREMENTS:
The software requirements specify the use of all required software products like editors , ide’s , compilers etc.  Each interface specifies the purpose of the interfacing software as related to this software product.  

 HTML 
 CSS
 JAVA SCRIPT. 
 PYTHON . 
 Flask 


Libraries Required

 Flask
 Nltk
 Pandas
 Matplotlib
 Mpld3
 Re
 Urllib
 Requests
 Pdfminer
 Docx
 Google Trans


 Design
 System Architecture 
A system architecture is the conceptual model that defines the structure, behavior, and more views of a system.  An architecture description is a formal description and representation of a system, organized in a way that supports reasoning about the structures and behaviors of the system. 
A system architecture can consist of system components and the sub-systems developed, that will work together to implement the overall system.  

 Testing 
Testing is a group of techniques to determine the correctness of the application under the predefined script but, testing cannot find all the defect of application.  The main intent of testing is to detect failures of the application so that failures can be discovered and corrected.  It does not demonstrate that a product functions properly under all conditions but only that it is not working in some specific conditions. 
Testing furnishes comparison that compares the behavior and state of software against mechanisms because the problem can be recognized by the mechanism.  The mechanism may include past versions of the same specified product, comparable products, and interfaces of expected purpose, relevant standards, or other criteria but not limited up to these. 
Testing includes an examination of code and also the execution of code in various environments, conditions as well as all the examining aspects of the code.  In the current scenario of software development, a testing team may be separate from the development team so that Information derived from testing can be used to correct the process of software development. 
The success of software depends upon acceptance of its targeted audience, easy graphical user interface, strong functionality load test, etc. 

Manual testing is done in this project . 

The process of checking the functionality of an application as per the customer needs without taking any help of automation tools is known as manual testing.  While performing the manual testing on any application, we do not need any specific knowledge of any testing tool, rather than have a proper understanding of the product so we can easily prepare the test document. 


White Box Testing
In white-box testing, the developer will inspect every line of code before handing it over to the testing team or the concerned test engineers. 
Subsequently, the code is noticeable for developers throughout testing; that's why this process is known as WBT (White Box Testing). 
In other words, we can say that the developer will execute the complete white-box testing for the particular software and send the specific application to the testing team. 
The purpose of implementing the white box testing is to emphasize the flow of inputs and outputs over the software and enhance the security of an application. 
In white box testing the following tests are done :
 Path testing
 Loop testing
 Condition testing
 Testing based on the memory perspective
 Test performance of the program

 Black box testing
Black box testing is a technique of software testing which examines the functionality of software without peering into its internal structure or coding.  The primary source of black box testing is a specification of requirements that is stated by the customer. 
In this method, tester selects a function and gives input value to examine its functionality, and checks whether the function is giving expected output or not.  If the function produces correct output, then it is passed in testing, otherwise failed.  The test team reports the result to the development team and then tests the next function.  After completing testing of all functions if there are severe problems, then it is given back to the development team for correction. 



Decision table technique in Black box testing
Input by user	T	T	F	F
Correct format	T	F	T	F
Expected Behaviour	Visualizations page /
Answer page	Error page	Not possible	Not possible

 Functional Testing
It is a type of software testing which is used to verify the functionality of the software application, whether the function is working according to the requirement specification.  In functional testing, each function tested by giving the value, determining the output, and verifying the actual output with the expected value.  Functional testing performed as black-box testing which is presented to confirm that the functionality of an application or system behaves as we are expecting.  It is done to verify the functionality of the application. 
The purpose of the functional testing is to check the primary entry function, necessarily usable function, the flow of screen GUI.  Functional testing displays the error message so that the user can easily navigate throughout the application. 

Unit Testing
Unit testing involves the testing of each unit or an individual component of the software application.  It is the first level of functional testing.  The aim behind unit testing is to validate unit components with its performance. 
A unit is a single testable part of a software system and tested during the development phase of the application software. 
The purpose of unit testing is to test the correctness of isolated code.  A unit component is an individual function or code of the application.  White box testing approach used for unit testing and usually done by the developers. 
Whenever the application is ready and given to the Test engineer, he/she will start checking every component of the module or module of the application independently or one by one, and this process is known as Unit testing or components testing. 

 Integration testing
Integration testing is the second level of the software testing process comes after unit testing.  In this testing, units or individual components of the software are tested in a group.  The focus of the integration testing level is to expose defects at the time of interaction between integrated components or units. 
Unit testing uses modules for testing purpose, and these modules are combined and tested in integration testing.  The Software is developed with a number of software modules that are coded by different coders or programmers.  The goal of integration testing is to check the correctness of communication among all the modules. 
 System Testing
System Testing includes testing of a fully integrated software system.  Generally, a computer system is made with the integration of software (any software is only a single element of a computer system).  The software is developed in units and then interfaced with other software and hardware to create a complete computer system.  In other words, a computer system consists of a group of software to perform the various tasks, but only software cannot perform the task; for that software must be interfaced with compatible hardware.  System testing is a series of different type of tests with the purpose to exercise and examine the full working of an integrated software computer system against requirements. 

To check the end-to-end flow of an application or the software as a user is known as System testing.  In this, we navigate (go through) all the necessary modules of an application and check if the end features or the end business works fine, and test the product as a whole system. 
It is end-to-end testing where the testing environment is similar to the production environment


 Testing
 Acceptance testing
Acceptance testing is formal testing based on user requirements and function processing.  It determines whether the software is conforming specified requirements and user requirements or not.  It is conducted as a kind of Black Box testing where the number of required users involved testing the acceptance level of the system. User acceptance testing (UAT) is a type of testing, which is done by the customer before accepting the final product.  Generally, UAT is done by the customer (domain expert) for their satisfaction, and check whether the application is working according to given business scenarios, real-time scenarios. 
In this, we concentrate only on those features and scenarios which are regularly used by the customer or mostly user scenarios for the business or those scenarios which are used daily by the end-user or the customer. 
However, the software has passed through three testing levels (Unit Testing, Integration Testing, System Testing) But still there are some minor errors which can be identified when the system is used by the end user in the actual scenario. 
Acceptance testing is the squeezing of all the testing processes that have done previously. 
 Test Cases

Test case id	Test case name	Test case description	Steps	Expected	actual	Priority
1	Upload file	Need to identify the file type 	Identify and extract 	Text is returned as output	Works fine	high
2	Insert link	Can scrape the data or not	Go to the web page and retrieve 	Text is returned	Works fine	High
3	Insert text	Able to take it	Receive it from user	Text received	Works fine	High
4	Insert keywords 	Will it process it and highlight in the context	Highlight each word in the context	Display highlighted words 	Works fine	Medium
5	Insert keyphrases	Will it process it and highlight in the context	Highlight each phrase in the context	Display highlighted phrases	Works fine	Medium
6	Pre process text 	Remove unwanted source related formatting	Remove using regular expressions 	Receive plain text without source formatting	There are small residues. 	Medium
7	Illegal web links	Check the behaviour	Give wrong links 	Error page 	Works fine	Low 
 Overheads of the Project
 Challenges
As the text summarization provides the reduced subtext of the original text there are always a lot of challenges there measure if we get the required text summary from the large document or not.  If, the summary contains important sentences and words or not.  Some of the challenge of text summarization are discussed as follow:

Extract hidden semantic relationship
In the text, there are many sentences that are related to each other.  The text includes many sentences.  Some sentences in the document related to each based on have semantic relationships between concepts in the text.  Many sentences are related to each other that depict some particular details of the text document.  So, capturing such sentences in the summary is always challenging task. 

Relevance detection
While generating a summary of the text it is also important to find the relevant sentences in our text documents so that they can be selected to make up the final summaries.  This is always a challenging task to find out the relevant sentences in text so that they can be included in our final summary.  As it highly affects the quality of our summary.  “The Code Quality Principle” can be used to detect important sentences in our text document.  Different criterion like sentence position within the text, word and phrases frequencies and title overlap are some of the examples to ensure the relevance of the sentence.  

Appearance of pronouns during extractive text summarization which will make the summary unclear. 
If unnecessary topic is also present in the source text , there are chances of getting them during the summary generation if topic is taken care then this problem can be taken care to some extent.  

 CONCLUSION & Future scope
As the availability of data in the form of text increasing day by day.  It becomes so difficult to read the whole textual data in order to find the required information which is both difficult as well as a time-consuming task for a human being.  So, at that time ATS performs an important role by providing a summary of a whole text document by extracting only the useful information and sentences.  There are different approaches of text summarization.  The real-world applications of text summarization can be: documents summarization, news and articles summarization, review systems, recommendation systems, social media monitoring, survey responses systems.  

The future goals of this project is to reduce the inefficiencies in the summaries by enhancing the the techniques used for overcoming the challenges as stated, by employing more effective methods.  Make much more useful visualizations for the gaining the best out of the context.  Better the techniques used for scraping the web during text extraction form internet.  Train state of the art models for extractive answering and query resolution for appropriate answering of the questions. The ultimate aim is fine tune every aspect of the project and make overall performance as close as possible to humans.   
